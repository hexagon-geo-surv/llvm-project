// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --check-attributes --check-globals all --include-generated-funcs --prefix-filecheck-ir-name VAR --version 6
// REQUIRES: amdgpu-registered-target

// RUN: %clang_cc1 -fopenmp -x c++ -std=c++11 -triple x86_64-unknown-unknown -fopenmp-targets=amdgcn-amd-amdhsa -emit-llvm-bc %s -o %t-ppc-host.bc
// RUN: %clang_cc1 -fopenmp -x c++ -std=c++11 -triple amdgcn-amd-amdhsa -fopenmp-targets=amdgcn-amd-amdhsa -emit-llvm %s -fopenmp-is-target-device -fopenmp-host-ir-file-path %t-ppc-host.bc -o - | FileCheck -check-prefix=DEFAULT %s
// RUN: %clang_cc1 -target-cpu gfx900 -fopenmp -x c++ -std=c++11 -triple amdgcn-amd-amdhsa -fopenmp-targets=amdgcn-amd-amdhsa -emit-llvm %s -fopenmp-is-target-device -fopenmp-host-ir-file-path %t-ppc-host.bc -o - | FileCheck -check-prefix=CPU %s

// RUN: %clang_cc1 -menable-no-nans -mno-amdgpu-ieee -fopenmp -x c++ -std=c++11 -triple amdgcn-amd-amdhsa -fopenmp-targets=amdgcn-amd-amdhsa -emit-llvm %s -fopenmp-is-target-device -fopenmp-host-ir-file-path %t-ppc-host.bc -o - | FileCheck -check-prefix=NOIEEE %s

// expected-no-diagnostics

#define N 100

int callable(int);

// Check that the target attributes are set on the generated kernel
int func() {

  int arr[N];

#pragma omp target teams thread_limit(42)
  for (int i = 0; i < N; i++) {
    arr[i] = callable(arr[i]);
  }

  return arr[0];
}

int callable(int x) {
  // ALL-LABEL: @_Z8callablei(i32 noundef %x) #2
  return x + 1;
}
//.
// DEFAULT: @__omp_rtl_debug_kind = weak_odr hidden addrspace(1) constant i32 0
// DEFAULT: @__omp_rtl_assume_teams_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// DEFAULT: @__omp_rtl_assume_threads_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// DEFAULT: @__omp_rtl_assume_no_thread_state = weak_odr hidden addrspace(1) constant i32 0
// DEFAULT: @__omp_rtl_assume_no_nested_parallelism = weak_odr hidden addrspace(1) constant i32 0
// DEFAULT: @[[GLOB0:[0-9]+]] = private unnamed_addr addrspace(1) constant [23 x i8] c"
// DEFAULT: @[[GLOB1:[0-9]+]] = private unnamed_addr addrspace(1) constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr addrspacecast (ptr addrspace(1) @[[GLOB0]] to ptr) }, align 8
// DEFAULT: @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment = weak_odr protected addrspace(1) global %struct.DynamicEnvironmentTy zeroinitializer
// DEFAULT: @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment = weak_odr protected addrspace(1) constant %struct.KernelEnvironmentTy { %struct.ConfigurationEnvironmentTy { i8 1, i8 1, i8 1, i32 1, i32 42, i32 0, i32 0, i32 0, i32 0 }, ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr), ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment to ptr) }
//.
// CPU: @__omp_rtl_debug_kind = weak_odr hidden addrspace(1) constant i32 0
// CPU: @__omp_rtl_assume_teams_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// CPU: @__omp_rtl_assume_threads_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// CPU: @__omp_rtl_assume_no_thread_state = weak_odr hidden addrspace(1) constant i32 0
// CPU: @__omp_rtl_assume_no_nested_parallelism = weak_odr hidden addrspace(1) constant i32 0
// CPU: @[[GLOB0:[0-9]+]] = private unnamed_addr addrspace(1) constant [23 x i8] c"
// CPU: @[[GLOB1:[0-9]+]] = private unnamed_addr addrspace(1) constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr addrspacecast (ptr addrspace(1) @[[GLOB0]] to ptr) }, align 8
// CPU: @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment = weak_odr protected addrspace(1) global %struct.DynamicEnvironmentTy zeroinitializer
// CPU: @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment = weak_odr protected addrspace(1) constant %struct.KernelEnvironmentTy { %struct.ConfigurationEnvironmentTy { i8 1, i8 1, i8 1, i32 1, i32 42, i32 0, i32 0, i32 0, i32 0 }, ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr), ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment to ptr) }
//.
// NOIEEE: @__omp_rtl_debug_kind = weak_odr hidden addrspace(1) constant i32 0
// NOIEEE: @__omp_rtl_assume_teams_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// NOIEEE: @__omp_rtl_assume_threads_oversubscription = weak_odr hidden addrspace(1) constant i32 0
// NOIEEE: @__omp_rtl_assume_no_thread_state = weak_odr hidden addrspace(1) constant i32 0
// NOIEEE: @__omp_rtl_assume_no_nested_parallelism = weak_odr hidden addrspace(1) constant i32 0
// NOIEEE: @[[GLOB0:[0-9]+]] = private unnamed_addr addrspace(1) constant [23 x i8] c"
// NOIEEE: @[[GLOB1:[0-9]+]] = private unnamed_addr addrspace(1) constant %struct.ident_t { i32 0, i32 2, i32 0, i32 22, ptr addrspacecast (ptr addrspace(1) @[[GLOB0]] to ptr) }, align 8
// NOIEEE: @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment = weak_odr protected addrspace(1) global %struct.DynamicEnvironmentTy zeroinitializer
// NOIEEE: @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment = weak_odr protected addrspace(1) constant %struct.KernelEnvironmentTy { %struct.ConfigurationEnvironmentTy { i8 1, i8 1, i8 1, i32 1, i32 42, i32 0, i32 0, i32 0, i32 0 }, ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr), ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_dynamic_environment to ptr) }
//.
// DEFAULT: Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
// DEFAULT-LABEL: define weak_odr protected amdgpu_kernel void @__omp_offloading_10302_8f219df__Z4funcv_l21(
// DEFAULT-SAME: ptr noalias noundef [[DYN_PTR:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR0:[0-9]+]] {
// DEFAULT-NEXT:  [[ENTRY:.*:]]
// DEFAULT-NEXT:    [[DYN_PTR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// DEFAULT-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// DEFAULT-NEXT:    [[DOTZERO_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// DEFAULT-NEXT:    [[DOTTHREADID_TEMP_:%.*]] = alloca i32, align 4, addrspace(5)
// DEFAULT-NEXT:    [[DYN_PTR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DYN_PTR_ADDR]] to ptr
// DEFAULT-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// DEFAULT-NEXT:    [[DOTZERO_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTZERO_ADDR]] to ptr
// DEFAULT-NEXT:    [[DOTTHREADID_TEMP__ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTTHREADID_TEMP_]] to ptr
// DEFAULT-NEXT:    store ptr [[DYN_PTR]], ptr [[DYN_PTR_ADDR_ASCAST]], align 8
// DEFAULT-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// DEFAULT-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6:![0-9]+]], !align [[META7:![0-9]+]]
// DEFAULT-NEXT:    [[TMP1:%.*]] = call i32 @__kmpc_target_init(ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment to ptr), ptr [[DYN_PTR]])
// DEFAULT-NEXT:    [[EXEC_USER_CODE:%.*]] = icmp eq i32 [[TMP1]], -1
// DEFAULT-NEXT:    br i1 [[EXEC_USER_CODE]], label %[[USER_CODE_ENTRY:.*]], label %[[WORKER_EXIT:.*]]
// DEFAULT:       [[USER_CODE_ENTRY]]:
// DEFAULT-NEXT:    [[TMP2:%.*]] = call i32 @__kmpc_global_thread_num(ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr))
// DEFAULT-NEXT:    store i32 0, ptr [[DOTZERO_ADDR_ASCAST]], align 4
// DEFAULT-NEXT:    store i32 [[TMP2]], ptr [[DOTTHREADID_TEMP__ASCAST]], align 4
// DEFAULT-NEXT:    call void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(ptr [[DOTTHREADID_TEMP__ASCAST]], ptr [[DOTZERO_ADDR_ASCAST]], ptr [[TMP0]]) #[[ATTR3:[0-9]+]]
// DEFAULT-NEXT:    call void @__kmpc_target_deinit()
// DEFAULT-NEXT:    ret void
// DEFAULT:       [[WORKER_EXIT]]:
// DEFAULT-NEXT:    ret void
//
//
// DEFAULT: Function Attrs: convergent noinline norecurse nounwind optnone
// DEFAULT-LABEL: define internal void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(
// DEFAULT-SAME: ptr noalias noundef [[DOTGLOBAL_TID_:%.*]], ptr noalias noundef [[DOTBOUND_TID_:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR1:[0-9]+]] {
// DEFAULT-NEXT:  [[ENTRY:.*:]]
// DEFAULT-NEXT:    [[DOTGLOBAL_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// DEFAULT-NEXT:    [[DOTBOUND_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// DEFAULT-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// DEFAULT-NEXT:    [[I:%.*]] = alloca i32, align 4, addrspace(5)
// DEFAULT-NEXT:    [[DOTGLOBAL_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTGLOBAL_TID__ADDR]] to ptr
// DEFAULT-NEXT:    [[DOTBOUND_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTBOUND_TID__ADDR]] to ptr
// DEFAULT-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// DEFAULT-NEXT:    [[I_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[I]] to ptr
// DEFAULT-NEXT:    store ptr [[DOTGLOBAL_TID_]], ptr [[DOTGLOBAL_TID__ADDR_ASCAST]], align 8
// DEFAULT-NEXT:    store ptr [[DOTBOUND_TID_]], ptr [[DOTBOUND_TID__ADDR_ASCAST]], align 8
// DEFAULT-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// DEFAULT-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6]], !align [[META7]]
// DEFAULT-NEXT:    store i32 0, ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    br label %[[FOR_COND:.*]]
// DEFAULT:       [[FOR_COND]]:
// DEFAULT-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 100
// DEFAULT-NEXT:    br i1 [[CMP]], label %[[FOR_BODY:.*]], label %[[FOR_END:.*]]
// DEFAULT:       [[FOR_BODY]]:
// DEFAULT-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP2]] to i64
// DEFAULT-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM]]
// DEFAULT-NEXT:    [[TMP3:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// DEFAULT-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z8callablei(i32 noundef [[TMP3]]) #[[ATTR4:[0-9]+]]
// DEFAULT-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    [[IDXPROM1:%.*]] = sext i32 [[TMP4]] to i64
// DEFAULT-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM1]]
// DEFAULT-NEXT:    store i32 [[CALL]], ptr [[ARRAYIDX2]], align 4
// DEFAULT-NEXT:    br label %[[FOR_INC:.*]]
// DEFAULT:       [[FOR_INC]]:
// DEFAULT-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP5]], 1
// DEFAULT-NEXT:    store i32 [[INC]], ptr [[I_ASCAST]], align 4
// DEFAULT-NEXT:    br label %[[FOR_COND]], !llvm.loop [[LOOP8:![0-9]+]]
// DEFAULT:       [[FOR_END]]:
// DEFAULT-NEXT:    ret void
//
//
// DEFAULT: Function Attrs: convergent mustprogress noinline nounwind optnone
// DEFAULT-LABEL: define hidden noundef i32 @_Z8callablei(
// DEFAULT-SAME: i32 noundef [[X:%.*]]) #[[ATTR2:[0-9]+]] {
// DEFAULT-NEXT:  [[ENTRY:.*:]]
// DEFAULT-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4, addrspace(5)
// DEFAULT-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// DEFAULT-NEXT:    [[RETVAL_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL]] to ptr
// DEFAULT-NEXT:    [[X_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[X_ADDR]] to ptr
// DEFAULT-NEXT:    store i32 [[X]], ptr [[X_ADDR_ASCAST]], align 4
// DEFAULT-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR_ASCAST]], align 4
// DEFAULT-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1
// DEFAULT-NEXT:    ret i32 [[ADD]]
//
//
// CPU: Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
// CPU-LABEL: define weak_odr protected amdgpu_kernel void @__omp_offloading_10302_8f219df__Z4funcv_l21(
// CPU-SAME: ptr noalias noundef [[DYN_PTR:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR0:[0-9]+]] {
// CPU-NEXT:  [[ENTRY:.*:]]
// CPU-NEXT:    [[DYN_PTR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CPU-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CPU-NEXT:    [[DOTZERO_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CPU-NEXT:    [[DOTTHREADID_TEMP_:%.*]] = alloca i32, align 4, addrspace(5)
// CPU-NEXT:    [[DYN_PTR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DYN_PTR_ADDR]] to ptr
// CPU-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// CPU-NEXT:    [[DOTZERO_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTZERO_ADDR]] to ptr
// CPU-NEXT:    [[DOTTHREADID_TEMP__ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTTHREADID_TEMP_]] to ptr
// CPU-NEXT:    store ptr [[DYN_PTR]], ptr [[DYN_PTR_ADDR_ASCAST]], align 8
// CPU-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// CPU-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6:![0-9]+]], !align [[META7:![0-9]+]]
// CPU-NEXT:    [[TMP1:%.*]] = call i32 @__kmpc_target_init(ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment to ptr), ptr [[DYN_PTR]])
// CPU-NEXT:    [[EXEC_USER_CODE:%.*]] = icmp eq i32 [[TMP1]], -1
// CPU-NEXT:    br i1 [[EXEC_USER_CODE]], label %[[USER_CODE_ENTRY:.*]], label %[[WORKER_EXIT:.*]]
// CPU:       [[USER_CODE_ENTRY]]:
// CPU-NEXT:    [[TMP2:%.*]] = call i32 @__kmpc_global_thread_num(ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr))
// CPU-NEXT:    store i32 0, ptr [[DOTZERO_ADDR_ASCAST]], align 4
// CPU-NEXT:    store i32 [[TMP2]], ptr [[DOTTHREADID_TEMP__ASCAST]], align 4
// CPU-NEXT:    call void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(ptr [[DOTTHREADID_TEMP__ASCAST]], ptr [[DOTZERO_ADDR_ASCAST]], ptr [[TMP0]]) #[[ATTR3:[0-9]+]]
// CPU-NEXT:    call void @__kmpc_target_deinit()
// CPU-NEXT:    ret void
// CPU:       [[WORKER_EXIT]]:
// CPU-NEXT:    ret void
//
//
// CPU: Function Attrs: convergent noinline norecurse nounwind optnone
// CPU-LABEL: define internal void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(
// CPU-SAME: ptr noalias noundef [[DOTGLOBAL_TID_:%.*]], ptr noalias noundef [[DOTBOUND_TID_:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR1:[0-9]+]] {
// CPU-NEXT:  [[ENTRY:.*:]]
// CPU-NEXT:    [[DOTGLOBAL_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CPU-NEXT:    [[DOTBOUND_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CPU-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CPU-NEXT:    [[I:%.*]] = alloca i32, align 4, addrspace(5)
// CPU-NEXT:    [[DOTGLOBAL_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTGLOBAL_TID__ADDR]] to ptr
// CPU-NEXT:    [[DOTBOUND_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTBOUND_TID__ADDR]] to ptr
// CPU-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// CPU-NEXT:    [[I_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[I]] to ptr
// CPU-NEXT:    store ptr [[DOTGLOBAL_TID_]], ptr [[DOTGLOBAL_TID__ADDR_ASCAST]], align 8
// CPU-NEXT:    store ptr [[DOTBOUND_TID_]], ptr [[DOTBOUND_TID__ADDR_ASCAST]], align 8
// CPU-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// CPU-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6]], !align [[META7]]
// CPU-NEXT:    store i32 0, ptr [[I_ASCAST]], align 4
// CPU-NEXT:    br label %[[FOR_COND:.*]]
// CPU:       [[FOR_COND]]:
// CPU-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// CPU-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 100
// CPU-NEXT:    br i1 [[CMP]], label %[[FOR_BODY:.*]], label %[[FOR_END:.*]]
// CPU:       [[FOR_BODY]]:
// CPU-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// CPU-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP2]] to i64
// CPU-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM]]
// CPU-NEXT:    [[TMP3:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// CPU-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z8callablei(i32 noundef [[TMP3]]) #[[ATTR4:[0-9]+]]
// CPU-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// CPU-NEXT:    [[IDXPROM1:%.*]] = sext i32 [[TMP4]] to i64
// CPU-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM1]]
// CPU-NEXT:    store i32 [[CALL]], ptr [[ARRAYIDX2]], align 4
// CPU-NEXT:    br label %[[FOR_INC:.*]]
// CPU:       [[FOR_INC]]:
// CPU-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// CPU-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP5]], 1
// CPU-NEXT:    store i32 [[INC]], ptr [[I_ASCAST]], align 4
// CPU-NEXT:    br label %[[FOR_COND]], !llvm.loop [[LOOP8:![0-9]+]]
// CPU:       [[FOR_END]]:
// CPU-NEXT:    ret void
//
//
// CPU: Function Attrs: convergent mustprogress noinline nounwind optnone
// CPU-LABEL: define hidden noundef i32 @_Z8callablei(
// CPU-SAME: i32 noundef [[X:%.*]]) #[[ATTR2:[0-9]+]] {
// CPU-NEXT:  [[ENTRY:.*:]]
// CPU-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4, addrspace(5)
// CPU-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CPU-NEXT:    [[RETVAL_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL]] to ptr
// CPU-NEXT:    [[X_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[X_ADDR]] to ptr
// CPU-NEXT:    store i32 [[X]], ptr [[X_ADDR_ASCAST]], align 4
// CPU-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR_ASCAST]], align 4
// CPU-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1
// CPU-NEXT:    ret i32 [[ADD]]
//
//
// NOIEEE: Function Attrs: convergent mustprogress noinline norecurse nounwind optnone
// NOIEEE-LABEL: define weak_odr protected amdgpu_kernel void @__omp_offloading_10302_8f219df__Z4funcv_l21(
// NOIEEE-SAME: ptr noalias noundef [[DYN_PTR:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR0:[0-9]+]] {
// NOIEEE-NEXT:  [[ENTRY:.*:]]
// NOIEEE-NEXT:    [[DYN_PTR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// NOIEEE-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// NOIEEE-NEXT:    [[DOTZERO_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// NOIEEE-NEXT:    [[DOTTHREADID_TEMP_:%.*]] = alloca i32, align 4, addrspace(5)
// NOIEEE-NEXT:    [[DYN_PTR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DYN_PTR_ADDR]] to ptr
// NOIEEE-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// NOIEEE-NEXT:    [[DOTZERO_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTZERO_ADDR]] to ptr
// NOIEEE-NEXT:    [[DOTTHREADID_TEMP__ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTTHREADID_TEMP_]] to ptr
// NOIEEE-NEXT:    store ptr [[DYN_PTR]], ptr [[DYN_PTR_ADDR_ASCAST]], align 8
// NOIEEE-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// NOIEEE-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6:![0-9]+]], !align [[META7:![0-9]+]]
// NOIEEE-NEXT:    [[TMP1:%.*]] = call i32 @__kmpc_target_init(ptr addrspacecast (ptr addrspace(1) @__omp_offloading_10302_8f219df__Z4funcv_l21_kernel_environment to ptr), ptr [[DYN_PTR]])
// NOIEEE-NEXT:    [[EXEC_USER_CODE:%.*]] = icmp eq i32 [[TMP1]], -1
// NOIEEE-NEXT:    br i1 [[EXEC_USER_CODE]], label %[[USER_CODE_ENTRY:.*]], label %[[WORKER_EXIT:.*]]
// NOIEEE:       [[USER_CODE_ENTRY]]:
// NOIEEE-NEXT:    [[TMP2:%.*]] = call i32 @__kmpc_global_thread_num(ptr addrspacecast (ptr addrspace(1) @[[GLOB1]] to ptr))
// NOIEEE-NEXT:    store i32 0, ptr [[DOTZERO_ADDR_ASCAST]], align 4
// NOIEEE-NEXT:    store i32 [[TMP2]], ptr [[DOTTHREADID_TEMP__ASCAST]], align 4
// NOIEEE-NEXT:    call void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(ptr [[DOTTHREADID_TEMP__ASCAST]], ptr [[DOTZERO_ADDR_ASCAST]], ptr [[TMP0]]) #[[ATTR3:[0-9]+]]
// NOIEEE-NEXT:    call void @__kmpc_target_deinit()
// NOIEEE-NEXT:    ret void
// NOIEEE:       [[WORKER_EXIT]]:
// NOIEEE-NEXT:    ret void
//
//
// NOIEEE: Function Attrs: convergent noinline norecurse nounwind optnone
// NOIEEE-LABEL: define internal void @__omp_offloading_10302_8f219df__Z4funcv_l21_omp_outlined(
// NOIEEE-SAME: ptr noalias noundef [[DOTGLOBAL_TID_:%.*]], ptr noalias noundef [[DOTBOUND_TID_:%.*]], ptr noundef nonnull align 4 dereferenceable(400) [[ARR:%.*]]) #[[ATTR1:[0-9]+]] {
// NOIEEE-NEXT:  [[ENTRY:.*:]]
// NOIEEE-NEXT:    [[DOTGLOBAL_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// NOIEEE-NEXT:    [[DOTBOUND_TID__ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// NOIEEE-NEXT:    [[ARR_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// NOIEEE-NEXT:    [[I:%.*]] = alloca i32, align 4, addrspace(5)
// NOIEEE-NEXT:    [[DOTGLOBAL_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTGLOBAL_TID__ADDR]] to ptr
// NOIEEE-NEXT:    [[DOTBOUND_TID__ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[DOTBOUND_TID__ADDR]] to ptr
// NOIEEE-NEXT:    [[ARR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[ARR_ADDR]] to ptr
// NOIEEE-NEXT:    [[I_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[I]] to ptr
// NOIEEE-NEXT:    store ptr [[DOTGLOBAL_TID_]], ptr [[DOTGLOBAL_TID__ADDR_ASCAST]], align 8
// NOIEEE-NEXT:    store ptr [[DOTBOUND_TID_]], ptr [[DOTBOUND_TID__ADDR_ASCAST]], align 8
// NOIEEE-NEXT:    store ptr [[ARR]], ptr [[ARR_ADDR_ASCAST]], align 8
// NOIEEE-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARR_ADDR_ASCAST]], align 8, !nonnull [[META6]], !align [[META7]]
// NOIEEE-NEXT:    store i32 0, ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    br label %[[FOR_COND:.*]]
// NOIEEE:       [[FOR_COND]]:
// NOIEEE-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 100
// NOIEEE-NEXT:    br i1 [[CMP]], label %[[FOR_BODY:.*]], label %[[FOR_END:.*]]
// NOIEEE:       [[FOR_BODY]]:
// NOIEEE-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP2]] to i64
// NOIEEE-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM]]
// NOIEEE-NEXT:    [[TMP3:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// NOIEEE-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z8callablei(i32 noundef [[TMP3]]) #[[ATTR4:[0-9]+]]
// NOIEEE-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    [[IDXPROM1:%.*]] = sext i32 [[TMP4]] to i64
// NOIEEE-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [100 x i32], ptr [[TMP0]], i64 0, i64 [[IDXPROM1]]
// NOIEEE-NEXT:    store i32 [[CALL]], ptr [[ARRAYIDX2]], align 4
// NOIEEE-NEXT:    br label %[[FOR_INC:.*]]
// NOIEEE:       [[FOR_INC]]:
// NOIEEE-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP5]], 1
// NOIEEE-NEXT:    store i32 [[INC]], ptr [[I_ASCAST]], align 4
// NOIEEE-NEXT:    br label %[[FOR_COND]], !llvm.loop [[LOOP8:![0-9]+]]
// NOIEEE:       [[FOR_END]]:
// NOIEEE-NEXT:    ret void
//
//
// NOIEEE: Function Attrs: convergent mustprogress noinline nounwind optnone
// NOIEEE-LABEL: define hidden noundef i32 @_Z8callablei(
// NOIEEE-SAME: i32 noundef [[X:%.*]]) #[[ATTR2:[0-9]+]] {
// NOIEEE-NEXT:  [[ENTRY:.*:]]
// NOIEEE-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4, addrspace(5)
// NOIEEE-NEXT:    [[X_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// NOIEEE-NEXT:    [[RETVAL_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL]] to ptr
// NOIEEE-NEXT:    [[X_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[X_ADDR]] to ptr
// NOIEEE-NEXT:    store i32 [[X]], ptr [[X_ADDR_ASCAST]], align 4
// NOIEEE-NEXT:    [[TMP0:%.*]] = load i32, ptr [[X_ADDR_ASCAST]], align 4
// NOIEEE-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP0]], 1
// NOIEEE-NEXT:    ret i32 [[ADD]]
//
//.
// DEFAULT: attributes #[[ATTR0]] = { convergent mustprogress noinline norecurse nounwind optnone "amdgpu-flat-work-group-size"="1,42" "kernel" "no-trapping-math"="true" "omp_target_thread_limit"="42" "stack-protector-buffer-size"="8" "uniform-work-group-size"="true" }
// DEFAULT: attributes #[[ATTR1]] = { convergent noinline norecurse nounwind optnone "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
// DEFAULT: attributes #[[ATTR2]] = { convergent mustprogress noinline nounwind optnone "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
// DEFAULT: attributes #[[ATTR3]] = { nounwind }
// DEFAULT: attributes #[[ATTR4]] = { convergent }
//.
// CPU: attributes #[[ATTR0]] = { convergent mustprogress noinline norecurse nounwind optnone "amdgpu-flat-work-group-size"="1,42" "kernel" "no-trapping-math"="true" "omp_target_thread_limit"="42" "stack-protector-buffer-size"="8" "target-cpu"="gfx900" "uniform-work-group-size"="true" }
// CPU: attributes #[[ATTR1]] = { convergent noinline norecurse nounwind optnone "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx900" }
// CPU: attributes #[[ATTR2]] = { convergent mustprogress noinline nounwind optnone "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx900" }
// CPU: attributes #[[ATTR3]] = { nounwind }
// CPU: attributes #[[ATTR4]] = { convergent }
//.
// NOIEEE: attributes #[[ATTR0]] = { convergent mustprogress noinline norecurse nounwind optnone "amdgpu-flat-work-group-size"="1,42" "amdgpu-ieee"="false" "kernel" "no-nans-fp-math"="true" "no-trapping-math"="true" "omp_target_thread_limit"="42" "stack-protector-buffer-size"="8" "uniform-work-group-size"="true" }
// NOIEEE: attributes #[[ATTR1]] = { convergent noinline norecurse nounwind optnone "no-nans-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
// NOIEEE: attributes #[[ATTR2]] = { convergent mustprogress noinline nounwind optnone "amdgpu-ieee"="false" "no-nans-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" }
// NOIEEE: attributes #[[ATTR3]] = { nounwind }
// NOIEEE: attributes #[[ATTR4]] = { convergent }
//.
// DEFAULT: [[META0:![0-9]+]] = !{i32 0, i32 66306, i32 150084063, !"_Z4funcv", i32 21, i32 0, i32 0}
// DEFAULT: [[META1:![0-9]+]] = !{i32 1, !"amdhsa_code_object_version", i32 600}
// DEFAULT: [[META2:![0-9]+]] = !{i32 1, !"wchar_size", i32 4}
// DEFAULT: [[META3:![0-9]+]] = !{i32 7, !"openmp", i32 51}
// DEFAULT: [[META4:![0-9]+]] = !{i32 7, !"openmp-device", i32 51}
// DEFAULT: [[META5:![0-9]+]] = !{!"{{.*}}clang version {{.*}}"}
// DEFAULT: [[META6]] = !{}
// DEFAULT: [[META7]] = !{i64 4}
// DEFAULT: [[LOOP8]] = distinct !{[[LOOP8]], [[META9:![0-9]+]]}
// DEFAULT: [[META9]] = !{!"llvm.loop.mustprogress"}
//.
// CPU: [[META0:![0-9]+]] = !{i32 0, i32 66306, i32 150084063, !"_Z4funcv", i32 21, i32 0, i32 0}
// CPU: [[META1:![0-9]+]] = !{i32 1, !"amdhsa_code_object_version", i32 600}
// CPU: [[META2:![0-9]+]] = !{i32 1, !"wchar_size", i32 4}
// CPU: [[META3:![0-9]+]] = !{i32 7, !"openmp", i32 51}
// CPU: [[META4:![0-9]+]] = !{i32 7, !"openmp-device", i32 51}
// CPU: [[META5:![0-9]+]] = !{!"{{.*}}clang version {{.*}}"}
// CPU: [[META6]] = !{}
// CPU: [[META7]] = !{i64 4}
// CPU: [[LOOP8]] = distinct !{[[LOOP8]], [[META9:![0-9]+]]}
// CPU: [[META9]] = !{!"llvm.loop.mustprogress"}
//.
// NOIEEE: [[META0:![0-9]+]] = !{i32 0, i32 66306, i32 150084063, !"_Z4funcv", i32 21, i32 0, i32 0}
// NOIEEE: [[META1:![0-9]+]] = !{i32 1, !"amdhsa_code_object_version", i32 600}
// NOIEEE: [[META2:![0-9]+]] = !{i32 1, !"wchar_size", i32 4}
// NOIEEE: [[META3:![0-9]+]] = !{i32 7, !"openmp", i32 51}
// NOIEEE: [[META4:![0-9]+]] = !{i32 7, !"openmp-device", i32 51}
// NOIEEE: [[META5:![0-9]+]] = !{!"{{.*}}clang version {{.*}}"}
// NOIEEE: [[META6]] = !{}
// NOIEEE: [[META7]] = !{i64 4}
// NOIEEE: [[LOOP8]] = distinct !{[[LOOP8]], [[META9:![0-9]+]]}
// NOIEEE: [[META9]] = !{!"llvm.loop.mustprogress"}
//.
